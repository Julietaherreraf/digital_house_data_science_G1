{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdCWqxqdprMN"
      },
      "source": [
        "# Proyecto Final - Grupo 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m5NpOPfmp24h"
      },
      "source": [
        "## Integrantes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKOZORV7p5Bd"
      },
      "source": [
        "* Franco Gaito\n",
        "* Francisco De Lorenzi\n",
        "* Julieta Herrera\n",
        "* Alejandro Relañez\n",
        "* Cristian Barrera"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOo9OuI90L-I"
      },
      "source": [
        "## Instalaciones"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOBtUBeJ0aC0"
      },
      "source": [
        "Ejecutar la primera vez y luego comentar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eS7n_i0M0LKD",
        "outputId": "f1fce683-8c66-4c6d-91ec-c9678295b7b5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: arch in /usr/local/lib/python3.10/dist-packages (7.0.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.10/dist-packages (from arch) (1.25.2)\n",
            "Requirement already satisfied: scipy>=1.8 in /usr/local/lib/python3.10/dist-packages (from arch) (1.11.4)\n",
            "Requirement already satisfied: pandas>=1.4 in /usr/local/lib/python3.10/dist-packages (from arch) (2.0.3)\n",
            "Requirement already satisfied: statsmodels>=0.12 in /usr/local/lib/python3.10/dist-packages (from arch) (0.14.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->arch) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->arch) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.4->arch) (2024.1)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12->arch) (0.5.6)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.12->arch) (24.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels>=0.12->arch) (1.16.0)\n",
            "Requirement already satisfied: pmdarima in /usr/local/lib/python3.10/dist-packages (2.0.4)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.4.2)\n",
            "Requirement already satisfied: Cython!=0.29.18,!=0.29.31,>=0.29 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (3.0.10)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.25.2)\n",
            "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (2.0.3)\n",
            "Requirement already satisfied: scikit-learn>=0.22 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.2.2)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (1.11.4)\n",
            "Requirement already satisfied: statsmodels>=0.13.2 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (0.14.2)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (2.0.7)\n",
            "Requirement already satisfied: setuptools!=50.0.0,>=38.6.0 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (67.7.2)\n",
            "Requirement already satisfied: packaging>=17.1 in /usr/local/lib/python3.10/dist-packages (from pmdarima) (24.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pmdarima) (2024.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.22->pmdarima) (3.5.0)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.10/dist-packages (from statsmodels>=0.13.2->pmdarima) (0.5.6)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from patsy>=0.5.6->statsmodels>=0.13.2->pmdarima) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        " !pip install arch\n",
        " !pip install pmdarima"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XmmwchzupzCg"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "H44Q0aF_qCkf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import statsmodels.api as sm\n",
        "import itertools\n",
        "import pmdarima as pm\n",
        "import warnings\n",
        "\n",
        "from typing import List, Dict, Any\n",
        "\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from statsmodels.tsa.seasonal import seasonal_decompose\n",
        "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
        "from statsmodels.tsa.stattools import acovf\n",
        "from statsmodels.tsa.stattools import adfuller, kpss\n",
        "\n",
        "from arch.unitroot import ZivotAndrews\n",
        "\n",
        "pd.set_option('display.max_rows', None)\n",
        "pd.set_option('display.max_columns', None)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2UBcfIhqDAH"
      },
      "source": [
        "## Funciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "At-IBvnVqFK4"
      },
      "outputs": [],
      "source": [
        "def dropear_columnas(df_x, columnas):\n",
        "    \"\"\"\n",
        "    Realiza modificaciones en un DataFrame dado, incluyendo eliminación de columnas específicas y\n",
        "    eliminación de filas con valores NaN en 'fake_positive'.\n",
        "    \"\"\"\n",
        "    df = df_x.copy()\n",
        "    df = df.drop(columns=columnas, axis=1)\n",
        "    return df\n",
        "\n",
        "\n",
        "def fix_types_and_nan(df: pd.DataFrame, config: Dict[str, Dict[str, Any]]) -> pd.DataFrame:\n",
        "    \"\"\"\n",
        "    Función para preprocesar un dataframe según una configuración proporcionada.\n",
        "    Rellena los valores NA y convierte los tipos de datos. Genera un error si se\n",
        "    encuentran valores NA en una columna donde 'na_fill_value' está configurado como\n",
        "    'NOT_ALLOWED'.\n",
        "\n",
        "    Nota: Devuelve un dataframe que SOLO contiene las columnas especificadas.\n",
        "\n",
        "    El parámetro config debe ser un diccionario que mapea nombres de columnas a\n",
        "    otro diccionario que especifica el tipo de dato para la columna ('dtype') y\n",
        "    cómo manejar los valores NA ('na_fill_value'). Si 'na_fill_value' está\n",
        "    configurado como 'NOT_ALLOWED', la función generará un ValueError si se\n",
        "    encuentran valores NA en esa columna.\n",
        "\n",
        "    Ejemplo:\n",
        "\n",
        "    config = {\n",
        "        'promotion_id': {'dtype': str, 'na_fill_value': 'NOT_ALLOWED'},\n",
        "        'days_of_week': {'dtype': str, 'na_fill_value': 'NOT_ALLOWED'},\n",
        "        'cap_amount': {'dtype': np.float32, 'na_fill_value': 0},\n",
        "        'slug': {'dtype': str, 'na_fill_value': 'NOT_ALLOWED'},\n",
        "        'promotion_banks': {'dtype': str, 'na_fill_value': 'NOT_ALLOWED'},\n",
        "        'pct_promo_value': {'dtype': np.float32, 'na_fill_value': 0},\n",
        "        'visibility_start_date': {'dtype': 'datetime64[ns]', 'na_fill_value': 'NOT_ALLOWED'},\n",
        "        'visibility_stop_date': {'dtype': 'datetime64[ns]', 'na_fill_value': 'NOT_ALLOWED'}\n",
        "    }\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    df : pandas.DataFrame\n",
        "        DataFrame de entrada a ser procesado.\n",
        "\n",
        "    config : dict\n",
        "        Diccionario que especifica los tipos de datos y el manejo de valores NA para cada columna.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    df : pandas.DataFrame\n",
        "        Dateframe procesado\n",
        "    \"\"\"\n",
        "    df_copy = df.copy()\n",
        "    for col, conf in config.items():\n",
        "        if col in df_copy.columns:\n",
        "            if conf['na_fill_value'] == \"NOT_ALLOWED\":\n",
        "                if df_copy[col].isnull().any():\n",
        "                    raise ValueError(f\"NA valores encontrados en columna {col} que no admite valores NA.\")\n",
        "                df_copy[col] = df_copy[col].astype(conf['dtype'])\n",
        "            else:\n",
        "                df_copy[col] = df_copy[col].fillna(conf['na_fill_value']).astype(conf['dtype'])\n",
        "        else:\n",
        "            raise KeyError(f\"{col} no encontrado en dataframe.\")\n",
        "\n",
        "    return df_copy[list(config.keys())]\n",
        "\n",
        "\n",
        "def plot_acf_pacf(df, title='FAS y FACP de la Serie Diferenciada'):\n",
        "    # Set up the plot\n",
        "    fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
        "    fig.suptitle(title)\n",
        "\n",
        "    # Plot ACF and PACF\n",
        "    plot_acf(df, ax=axes[0], title='FAS')\n",
        "    plot_pacf(df, ax=axes[1], title='FACP')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def plot_fac(df, title='FAC de la Serie Diferenciada'):\n",
        "    # Set up the plot\n",
        "    fig, ax = plt.subplots(figsize=(15, 5))\n",
        "    fig.suptitle(title)\n",
        "\n",
        "    # Plot FAC\n",
        "    fac_values = acovf(df, adjusted=True)\n",
        "    sns.lineplot(x=range(len(fac_values)), y=fac_values, ax=ax)\n",
        "    ax.set_title('FAC')\n",
        "\n",
        "    plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def stationarity_tests(df, title=''):\n",
        "    print(f'Stationarity Tests for: {title}')\n",
        "\n",
        "    # ADF Test\n",
        "    print('ADF Test:')\n",
        "    result_adf = adfuller(df, autolag='AIC')\n",
        "    labels_adf = ['ADF Test Statistic', 'p-value', '# Lags Used', '# Observations']\n",
        "    out_adf = pd.Series(result_adf[0:4], index=labels_adf)\n",
        "    for key, val in result_adf[4].items():\n",
        "        out_adf[f'Critical Value ({key})'] = val\n",
        "    print(out_adf.to_string())\n",
        "    print('\\n')\n",
        "\n",
        "    # KPSS Test\n",
        "    print('KPSS Test:')\n",
        "    result_kpss = kpss(df, regression='c', nlags='auto')\n",
        "    labels_kpss = ['KPSS Test Statistic', 'p-value', '# Lags Used']\n",
        "    out_kpss = pd.Series(result_kpss[0:3], index=labels_kpss)\n",
        "    for key, val in result_kpss[3].items():\n",
        "        out_kpss[f'Critical Value ({key})'] = val\n",
        "    print(out_kpss.to_string())\n",
        "    print('\\n')\n",
        "\n",
        "\n",
        "def test_stationarity(timeseries):\n",
        "    print('Stationarity Tests:')\n",
        "\n",
        "    rolmean = timeseries.rolling(12).mean()\n",
        "    rolstd = timeseries.rolling(12).std()\n",
        "    # Graficar:\n",
        "    orig = plt.plot(timeseries, color='blue',label='Original')\n",
        "    mean = plt.plot(rolmean, color='red', label='Rolling Mean')\n",
        "    std = plt.plot(rolstd, color='black', label = 'Rolling Std')\n",
        "    plt.legend(loc='best')\n",
        "    plt.title('Rolling Mean & Standard Deviation')\n",
        "    plt.show(block=False)\n",
        "\n",
        "    # Dickey-Fuller test:\n",
        "    print ('Results of Dickey-Fuller Test:')\n",
        "    timeseries = timeseries.iloc[:,0].values\n",
        "    dftest = adfuller(timeseries, autolag='AIC')\n",
        "    dfoutput = pd.Series(dftest[0:4], index=['Test Statistic','p-value','#Lags Used','Number of Observations Used'])\n",
        "    for key,value in dftest[4].items():\n",
        "        dfoutput['Critical Value (%s)'%key] = value\n",
        "    print(dfoutput)\n",
        "\n",
        "\n",
        "def tes_optimizer(train, test, abg, trend_mode='add', seasonal_mode='add', seasonal_period=12, step=5):\n",
        "    \"\"\"\n",
        "    This function optimizes hyperparameters for the TES model.\n",
        "    \"\"\"\n",
        "    best_alpha, best_beta, best_gamma, best_mae = None, None, None, float(\"inf\")\n",
        "    for comb in abg:\n",
        "        tes_model = ExponentialSmoothing(train, trend=trend_mode, seasonal=seasonal_mode, seasonal_periods=seasonal_period).\\\n",
        "            fit(smoothing_level=comb[0], smoothing_trend=comb[1], smoothing_seasonal=comb[2])\n",
        "        y_pred = tes_model.forecast(step)\n",
        "        mae = mean_absolute_error(test, y_pred)\n",
        "        if mae < best_mae:\n",
        "            best_alpha, best_beta, best_gamma, best_mae = comb[0], comb[1], comb[2], mae\n",
        "        print([round(comb[0], 2), round(comb[1], 2), round(comb[2], 2), round(mae, 2)])\n",
        "    print(\"best_alpha:\", round(best_alpha, 2), \"best_beta:\", round(best_beta, 2), \"best_gamma:\", round(best_gamma, 2),\n",
        "          \"best_mae:\", round(best_mae, 4))\n",
        "    return best_alpha, best_beta, best_gamma, best_mae\n",
        "\n",
        "\n",
        "def date_filter(df, column, inf, sup):\n",
        "    return df[ (df[column].dt.year >= inf) & (df[column].dt.year <= sup) ]\n",
        "\n",
        "\n",
        "def busqueda_sarimax(ts, valores_p, valores_d, valores_q, valores_P, valores_D, valores_Q, valores_s, exog=None):\n",
        "    \"\"\"\n",
        "    Realiza una búsqueda en cuadrícula para encontrar los mejores parámetros del modelo SARIMAX.\n",
        "\n",
        "    Parámetros:\n",
        "    ts (pd.Series): Datos de la serie temporal.\n",
        "    valores_p (list): Parte AR(p) del modelo.\n",
        "    valores_d (list): Parte I(d) del modelo.\n",
        "    valores_q (list): Parte MA(q) del modelo.\n",
        "    valores_P (list): Parte AR(P) estacional del modelo.\n",
        "    valores_D (list): Parte I(D) estacional del modelo.\n",
        "    valores_Q (list): Parte MA(Q) estacional del modelo.\n",
        "    valores_s (list): Períodos estacionales.\n",
        "    exog (pd.DataFrame, opcional): Variables exógenas.\n",
        "\n",
        "    Retorna:\n",
        "    dict: Mejores parámetros y el AIC correspondiente.\n",
        "    \"\"\"\n",
        "    warnings.filterwarnings(\"ignore\")\n",
        "    mejor_aic = float(\"inf\")\n",
        "    mejores_params = None\n",
        "\n",
        "    pdq = list(itertools.product(valores_p, valores_d, valores_q))\n",
        "    seasonal_pdq = list(itertools.product(valores_P, valores_D, valores_Q, valores_s))\n",
        "\n",
        "    for param in pdq:\n",
        "        for param_seasonal in seasonal_pdq:\n",
        "            try:\n",
        "                model = SARIMAX(ts, order=param, seasonal_order=param_seasonal, exog=exog)\n",
        "                results = model.fit(disp=False)\n",
        "                if results.aic < mejor_aic:\n",
        "                    mejor_aic = results.aic\n",
        "                    mejores_params = (param, param_seasonal)\n",
        "                print(f'SARIMAX{param}x{param_seasonal} - AIC:{results.aic}')\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "    return {\"params\": mejores_params, \"aic\": mejor_aic}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guOljJW8qFTi"
      },
      "source": [
        "## 1. Desarrollo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tz4DAtQ3EMah"
      },
      "source": [
        "**Breve descripción del proyecto**\n",
        "\n",
        "El proyecto se centra en el análisis de datos de aterrizajes y despegues de vuelos en Argentina, obtenidos del Ministerio de Transporte. El objetivo principal es comprender el comportamiento de los vuelos de Argentina a España para determinar la mejor fecha para viajar.\n",
        "\n",
        "Para lograr esto, se realizará un análisis exploratorio de los datos, se aplicarán feature engineering y se construirán modelos predictivos utilizando algoritmos de machine learning. El resultado final será proporcionar recomendaciones sobre las fechas óptimas para viajar basadas en patrones identificados en los datos históricos de vuelos.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "igHDnw8DqJdI"
      },
      "source": [
        "### 1.1) Importamos Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "HrIID2Yz4wwS",
        "outputId": "87a80ce7-b870-4925-b291-14f8aad71c6f"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../202112_informe_ministerio.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-3ed2580efaca>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Leer los archivos CSV con el delimitador ';'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf_vuelos_argentina_2021\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../202112_informe_ministerio.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf_vuelos_argentina_2022\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../202212-informe-ministerio.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mdf_vuelos_argentina_2023\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../202312-informe-ministerio-actualizado-dic.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdf_vuelos_argentina_2024\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../202404-informe-ministerio.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m';'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m    910\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    575\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    576\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 577\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    578\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1405\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1406\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1407\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1409\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1659\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1661\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1662\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1663\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    860\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../202112_informe_ministerio.csv'"
          ]
        }
      ],
      "source": [
        "# Leer los archivos CSV con el delimitador ';'\n",
        "df_vuelos_argentina_2021 = pd.read_csv('../202112_informe_ministerio.csv', delimiter=',')\n",
        "df_vuelos_argentina_2022 = pd.read_csv('../202212-informe-ministerio.csv', delimiter=';')\n",
        "df_vuelos_argentina_2023 = pd.read_csv('../202312-informe-ministerio-actualizado-dic.csv', delimiter=';')\n",
        "df_vuelos_argentina_2024 = pd.read_csv('../202404-informe-ministerio.csv', delimiter=';')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sJNhkQ8o2VZy"
      },
      "outputs": [],
      "source": [
        "# Renombrar la columna de fecha en df_vuelos_argentina_2021\n",
        "df_vuelos_argentina_2021.columns.values[0] = df_vuelos_argentina_2022.columns[0]\n",
        "\n",
        "# Combinar los DataFrames en uno solo\n",
        "df_vuelos_combinado = pd.concat([df_vuelos_argentina_2021, df_vuelos_argentina_2022, df_vuelos_argentina_2023, df_vuelos_argentina_2024], ignore_index=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kBD-4IwL56MS"
      },
      "outputs": [],
      "source": [
        "# Mostrar las primeras filas del DataFrame combinado\n",
        "df_vuelos_combinado.head()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v1C0d6LnsFyA"
      },
      "source": [
        "### 1.2) Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8oRpzX9He85"
      },
      "source": [
        "Renombramos columnas a snake_case"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTTCn69mQCVs"
      },
      "outputs": [],
      "source": [
        "df_vuelos_combinado.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a-dl4sur801r"
      },
      "outputs": [],
      "source": [
        "df_vuelos_combinado.rename(columns=lambda x: x.lower().replace(' ', '_'), inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ikAhvqWMGjAj"
      },
      "outputs": [],
      "source": [
        "df_vuelos_combinado.rename(columns={'clase_de_vuelo_(todos_los_vuelos)': 'clase_de_vuelo'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVWTvTS7HWwQ"
      },
      "outputs": [],
      "source": [
        "df_vuelos_combinado.rename(columns={'origen_/_destino': 'origen_destino'}, inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aWuy8cYI85cK"
      },
      "outputs": [],
      "source": [
        "df_vuelos_combinado.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FEAEZaHdryg8"
      },
      "outputs": [],
      "source": [
        "df_vuelos_combinado.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PGC-Pxe-RBsQ"
      },
      "outputs": [],
      "source": [
        "config = {\n",
        "    'fecha_utc': {'dtype': 'datetime64[ns]', 'na_fill_value': 'NOT_ALLOWED'},\n",
        "    'hora_utc': {'dtype': 'string', 'na_fill_value': 'NOT_ALLOWED'},\n",
        "    'clase_de_vuelo': {'dtype': 'string', 'na_fill_value': 'NOT_ALLOWED'},\n",
        "    'clasificación_vuelo': {'dtype': 'string', 'na_fill_value': 'NOT_ALLOWED'},\n",
        "    'tipo_de_movimiento': {'dtype': 'string', 'na_fill_value': 'NOT_ALLOWED'},\n",
        "    'aeropuerto': {'dtype': 'string', 'na_fill_value': 'NOT_ALLOWED'},\n",
        "    'origen_destino': {'dtype': 'string', 'na_fill_value': 'NOT_ALLOWED'},\n",
        "    'aerolinea_nombre': {'dtype': 'string', 'na_fill_value': 'NOT_ALLOWED'},\n",
        "    'aeronave': {'dtype': 'string', 'na_fill_value': 'NOT_ALLOWED'},\n",
        "    'pasajeros': {'dtype': 'int64', 'na_fill_value': 0},\n",
        "    'pax': {'dtype': 'string', 'na_fill_value': 'NOT_ALLOWED'},\n",
        "    'calidad_dato': {'dtype': 'string', 'na_fill_value': 'NOT_ALLOWED'}\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRmmZ1XdSQnk"
      },
      "outputs": [],
      "source": [
        "df_vuelos_combinado['fecha_utc'] = pd.to_datetime(df_vuelos_combinado['fecha_utc'], format='%d/%m/%Y', errors='coerce')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ack6ewVbSR-h"
      },
      "outputs": [],
      "source": [
        "df_vuelos_combinado['fecha_utc'].isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kAEn7vMnRObJ"
      },
      "outputs": [],
      "source": [
        "df_vuelos_combinado_correct_dtype = fix_types_and_nan(df_vuelos_combinado, config)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5rGB9-b4TQDQ"
      },
      "outputs": [],
      "source": [
        "df_vuelos_combinado_correct_dtype.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pFewrbVYrgqy"
      },
      "outputs": [],
      "source": [
        "# Extraer los años de la columna 'fecha'\n",
        "años = df_vuelos_combinado_correct_dtype['fecha_utc'].dt.year\n",
        "\n",
        "# Obtener los años únicos\n",
        "años_unicos = años.unique()\n",
        "\n",
        "# Mostrar los años únicos\n",
        "print(años_unicos)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhFR5LBpsP8N"
      },
      "outputs": [],
      "source": [
        "df_vuelos_combinado_correct_dtype.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZrvF0WKwsZwn"
      },
      "outputs": [],
      "source": [
        "df_vuelos_combinado_correct_dtype.isna().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ra_rWDlLwABb"
      },
      "outputs": [],
      "source": [
        "df_vuelos_combinado_correct_dtype.describe()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "255E-SKT3vOb"
      },
      "source": [
        "### 1.2) EDA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aU38jU27pZD"
      },
      "source": [
        "\n",
        "Buscamos entender el comportamiento y distribución de los datos del dataset para entender si tenemos suficiente información y cómo podemos utilizarla.\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1HpnwQwL79Ic"
      },
      "source": [
        "Analisis de clases de vuelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUPmu0FY34mV"
      },
      "outputs": [],
      "source": [
        "# Obtener la cuenta de cada clase de vuelo\n",
        "clase_vuelo_counts = df_vuelos_combinado_correct_dtype['clase_de_vuelo'].value_counts()\n",
        "\n",
        "# Crear un gráfico de barras\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=clase_vuelo_counts.index, y=clase_vuelo_counts.values, hue=clase_vuelo_counts.index, legend=False, palette='viridis')\n",
        "plt.title('Distribución de Clases de Vuelo en Argentina')\n",
        "plt.xlabel('')\n",
        "plt.ylabel('Cantidad de Vuelos')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKEfJQ9p8X4A"
      },
      "source": [
        "Análisis de clasificaciónes de vuelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DMarGhCI35uC"
      },
      "outputs": [],
      "source": [
        "# Obtener la cuenta de cada clasificación de vuelo\n",
        "clase_vuelo_counts = df_vuelos_combinado_correct_dtype['clasificación_vuelo'].value_counts()\n",
        "\n",
        "# Calcular los porcentajes sobre el total\n",
        "total_vuelos = len(df_vuelos_combinado_correct_dtype)\n",
        "porcentajes = (clase_vuelo_counts / total_vuelos) * 100\n",
        "\n",
        "# Crear una gráfica de barras\n",
        "plt.figure(figsize=(10, 5))\n",
        "barplot = sns.barplot(x=porcentajes.index, y=porcentajes.values, hue=porcentajes.index, legend=False, palette='viridis')\n",
        "\n",
        "# Agregar etiquetas de porcentaje sobre las barras\n",
        "for i in range(len(porcentajes)):\n",
        "    plt.annotate(f'{porcentajes[i]:.2f}%',\n",
        "                 xy=(i, porcentajes[i]),\n",
        "                 ha='center',\n",
        "                 va='bottom')\n",
        "\n",
        "plt.title('Distribución de Clasificación de Vuelo en Argentina')\n",
        "plt.xlabel('')\n",
        "plt.xticks(rotation=45)\n",
        "plt.gca().axes.get_yaxis().set_visible(False)  # Ocultar eje y\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QY3DhybB3-hV"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "**Conclusiones:**\n",
        "\n",
        "* La mayoría de los vuelos son domésticos\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2H2Xu4La8mui"
      },
      "source": [
        "\n",
        "\n",
        "Top 50 de columna origen/destino"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Vpf_x0N3562"
      },
      "outputs": [],
      "source": [
        "# Obtener el top 50 de la columna 'origen_destino'\n",
        "top_50_origen_destino = df_vuelos_combinado_correct_dtype['origen_destino'].value_counts().head(50)\n",
        "\n",
        "# Mostrar los valores del top 5\n",
        "top_50_origen_destino.reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upN5Y0MUFeMV"
      },
      "source": [
        "**Conclusiones:**\n",
        "\n",
        "Analizando el top 50 de vuelos origen/destino podemos ver que LEMD (Madrid) se encuentra en el puesto 39, sería conveniente utilizar este destino para realizar el pronóstico de viajes a España."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQ4wvY6rLI6w"
      },
      "source": [
        "Nos quedamos solo con vuelos a españa (Todos los destinos para comparar)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q_63XgUjLIGs"
      },
      "outputs": [],
      "source": [
        "# Filtrar el DataFrame para incluir solo vuelos entre Argentina y España\n",
        "aeropuertos=['LEMD', 'LEBL', 'MAD']\n",
        "df_vuelos_argentina_españa_todos_los_destinos = df_vuelos_combinado_correct_dtype[df_vuelos_combinado_correct_dtype['origen_destino'].isin(aeropuertos)]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P-9yVV-N359t"
      },
      "outputs": [],
      "source": [
        "# Contar la cantidad de vuelos por aeropuerto\n",
        "cantidad_vuelos_por_aeropuerto = df_vuelos_argentina_españa_todos_los_destinos['origen_destino'].value_counts()\n",
        "\n",
        "# Crear un gráfico de barras\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x=cantidad_vuelos_por_aeropuerto.index, y=cantidad_vuelos_por_aeropuerto.values, hue = cantidad_vuelos_por_aeropuerto.index, palette='viridis', legend=False)\n",
        "plt.title('Cantidad de Vuelos por Aeropuerto (Argentina y España)')\n",
        "plt.xlabel('Aeropuerto')\n",
        "plt.ylabel('Cantidad de Vuelos')\n",
        "for index, value in enumerate(cantidad_vuelos_por_aeropuerto):\n",
        "    barplot.text(index, value, str(value), ha='center', va='bottom', fontsize=8)\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TK3tVfjE36D_"
      },
      "outputs": [],
      "source": [
        "# Observamos un caso que nos llamó la atención de un vuelo a España desde Bahía Blanca. Observamos que se trata de un Jet privado\n",
        "análisis_puntual='BCA'\n",
        "df_vuelos_puntual = df_vuelos_combinado_correct_dtype[df_vuelos_combinado_correct_dtype[\"aeropuerto\"] == análisis_puntual]\n",
        "df_vuelos_puntual = df_vuelos_puntual[df_vuelos_puntual['origen_destino'].isin(aeropuertos)]\n",
        "df_vuelos_puntual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzSr8vZRMZfI"
      },
      "source": [
        "Filtramos solo vuelos a LEMD para nuestro análisis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "maVh1I3fMUVO"
      },
      "outputs": [],
      "source": [
        "# Filtramos el DataFrame para incluir solo vuelos entre Argentina y España ('LEMD')\n",
        "aeropuerto_elegido ='LEMD'\n",
        "df_vuelos_argentina_españa = df_vuelos_combinado_correct_dtype[df_vuelos_combinado_correct_dtype['origen_destino'].isin([aeropuerto_elegido])]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AwBVIPaoMtM9"
      },
      "outputs": [],
      "source": [
        "df_vuelos_argentina_españa.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AWh6Yf_cMwNa"
      },
      "outputs": [],
      "source": [
        "df_vuelos_argentina_españa.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XubdA0BOw7bK"
      },
      "source": [
        "Analizamos cantidad de pasajeros por mes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Vf65az84VhI"
      },
      "outputs": [],
      "source": [
        "monthly_passengers = df_vuelos_argentina_españa.copy()\n",
        "monthly_passengers.set_index('fecha_utc', inplace=True)\n",
        "monthly_passengers = monthly_passengers['pasajeros'].resample('M').sum()\n",
        "monthly_passengers.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wT8uP4_p4WiC"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "monthly_passengers.plot()\n",
        "plt.title('Número de pasajeros por mes')\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Número de pasajeros')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKv9unSQ4aUc"
      },
      "source": [
        "Nuestro objetivo es extrapolar la demanda según fecha con los precios de vuelo para poder predecir cuando volar más barato. En ese caso, nos enfocaremos en un caso práctico que es vuelos que conecten Argentina y España. Por ese motivo, para el estudio nos quedaremos con:\n",
        "\n",
        "*   Para el entrenamiento, vuelos entre 2021 y 2023. Para la prueba, vuelos de 2024\n",
        "*   Vuelos que involucren el aeropuerto español LEMD\n",
        "*   Vuelos comerciales\n",
        "*   Contemplamos tanto despegues como aterrizajes, es decir, IDA y VUELTA\n",
        "* Eliminamos filas con fecha o cantidad de pasajeros nulos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC7KugUwmAsA"
      },
      "outputs": [],
      "source": [
        "aeropuerto_españa = ['LEMD']\n",
        "clase_vuelo = 'Regular'\n",
        "\n",
        "# Filtrar el DataFrame original\n",
        "df_argentina_españa_comerciales = df_vuelos_combinado_correct_dtype[\n",
        "    (df_vuelos_combinado_correct_dtype['origen_destino'].isin(aeropuerto_españa)) &\n",
        "    (df_vuelos_combinado_correct_dtype['clase_de_vuelo'] == clase_vuelo)\n",
        "]\n",
        "\n",
        "num_vuelos = df_argentina_españa_comerciales.shape[0]\n",
        "print(f\"El número de vuelos en el dataset filtrado es de: {num_vuelos}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQxDHlodp-pw"
      },
      "outputs": [],
      "source": [
        "# Crear el dataset_train con registros entre 2021 y 2023\n",
        "lim_inferior_entrenamiento=2021\n",
        "lim_superior_entrenamiento=2023\n",
        "lim_inferior_prueba=2024\n",
        "lim_superior_prueba=2024\n",
        "\n",
        "dataset_train = df_argentina_españa_comerciales[\n",
        "    (df_argentina_españa_comerciales['fecha_utc'].dt.year >= lim_inferior_entrenamiento) &\n",
        "    (df_argentina_españa_comerciales['fecha_utc'].dt.year <= lim_superior_entrenamiento)\n",
        "]\n",
        "\n",
        "# Crear el dataset_test con registros del año 2024\n",
        "dataset_test = df_argentina_españa_comerciales[\n",
        "    (df_argentina_españa_comerciales['fecha_utc'].dt.year >= lim_inferior_prueba) &\n",
        "    (df_argentina_españa_comerciales['fecha_utc'].dt.year <= lim_superior_prueba)\n",
        "]\n",
        "\n",
        "# Imprimir el número de registros en cada dataset\n",
        "print(f\"El número de registros en dataset_train es: {len(dataset_train)}\")\n",
        "print(f\"El número de registros en dataset_test es: {len(dataset_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fO6ErY7mqFeK"
      },
      "source": [
        "Evaluamos gráficamente la cantidad de pasajeros por mes del DATASET de entrenamiento que contará con todos los mesese de los años 2021 a 2023, arrojandonos un resultado más concreto de como se desarrolla la variación en función del tiempo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBYM7OWeorPe"
      },
      "outputs": [],
      "source": [
        "monthly_passengers_filtering = dataset_train.copy()\n",
        "monthly_passengers_filtering.set_index('fecha_utc', inplace=True)\n",
        "monthly_passengers_filtering = monthly_passengers_filtering['pasajeros'].resample('M').sum()\n",
        "monthly_passengers_filtering.reset_index()\n",
        "monthly_passengers_filtering.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 228
        },
        "id": "Oqm5nfgWrCjB",
        "outputId": "5dfe3690-2b72-49b0-a1a5-02717aa8d2ca"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'monthly_passengers_filtering' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-827e7394c2ba>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmonthly_passengers_filtering\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Número de pasajeros por mes'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mxlabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Fecha'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mylabel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Número de pasajeros'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'monthly_passengers_filtering' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1500x500 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(15, 5))\n",
        "monthly_passengers_filtering.plot()\n",
        "plt.title('Número de pasajeros por mes')\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Número de pasajeros')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWKGd_GFr2Rj"
      },
      "source": [
        "Identificamos los aeropuertos argentinos, las aerolíneas y las aeronaves que guardan vínculo con estos viajes a España para vuelos comerciales en dicha fecha:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "egdBc9Rlr05E"
      },
      "outputs": [],
      "source": [
        "# Gráfico de barra para la columna 'aeropuerto'\n",
        "cantidad_vuelos_por_aeropuerto = df_argentina_españa_comerciales['aeropuerto'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=cantidad_vuelos_por_aeropuerto.index, y=cantidad_vuelos_por_aeropuerto.values, hue=cantidad_vuelos_por_aeropuerto.index, legend=False, palette='viridis')\n",
        "plt.title('Cantidad de Vuelos por Aeropuerto')\n",
        "plt.xlabel('Aeropuerto')\n",
        "plt.ylabel('Cantidad de Vuelos')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5YxgT8dVYFBd"
      },
      "outputs": [],
      "source": [
        "# Obtener el top 3 de la columna 'origen_destino'\n",
        "top_3_aerolineas = df_argentina_españa_comerciales['aerolinea_nombre'].value_counts().head(3)\n",
        "\n",
        "# Mostrar los valores del top 3\n",
        "top_3_aerolineas.reset_index()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jUuWiowYsuA"
      },
      "outputs": [],
      "source": [
        "# Reseteamos el índice y lo convertimos a lista\n",
        "lista_top_3 = top_3_aerolineas.reset_index().values.tolist()\n",
        "\n",
        "# Extraemos solo los nombres de las aerolíneas\n",
        "lista_top_3_names = top_3_aerolineas.index.tolist()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "61kuV3nGY573"
      },
      "outputs": [],
      "source": [
        "lista_top_3_names"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "24juke4YIw-t"
      },
      "outputs": [],
      "source": [
        "# Gráfico de barra para la columna 'aerolinea_nombre'\n",
        "cantidad_vuelos_por_aerolinea = df_argentina_españa_comerciales['aerolinea_nombre'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(16, 5))\n",
        "sns.barplot(x=cantidad_vuelos_por_aerolinea.index, y=cantidad_vuelos_por_aerolinea.values,hue=cantidad_vuelos_por_aerolinea.index, legend=False,palette='viridis')\n",
        "plt.title('Cantidad de Vuelos por Aerolínea')\n",
        "plt.xlabel('Aerolínea')\n",
        "plt.ylabel('Cantidad de Vuelos')\n",
        "plt.xticks(rotation=90)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eMQhE9PaI36h"
      },
      "outputs": [],
      "source": [
        "# Gráfico de barra para la columna 'aeronave'\n",
        "cantidad_vuelos_por_aeronave = df_argentina_españa_comerciales['aeronave'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "sns.barplot(x=cantidad_vuelos_por_aeronave.index, y=cantidad_vuelos_por_aeronave.values,hue=cantidad_vuelos_por_aeronave.index,legend=False, palette='viridis')\n",
        "plt.title('Cantidad de Vuelos por Aeronave')\n",
        "plt.xlabel('Aeronave')\n",
        "plt.ylabel('Cantidad de Vuelos')\n",
        "plt.xticks(rotation=45)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip_rBD_kXp2S"
      },
      "source": [
        "Elegimos el par EZE - LEMD"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XDBLdZSIasuW"
      },
      "outputs": [],
      "source": [
        "aeropuerto_españa = ['LEMD']\n",
        "clase_vuelo = 'Regular'\n",
        "aeropuerto_base = 'EZE'\n",
        "\n",
        "# Filtrar el DataFrame original\n",
        "df_vuelos_pre_final = df_vuelos_combinado_correct_dtype[\n",
        "    (df_vuelos_combinado_correct_dtype['origen_destino'].isin(aeropuerto_españa)) &\n",
        "    (df_vuelos_combinado_correct_dtype['clase_de_vuelo'] == clase_vuelo) &\n",
        "    (df_vuelos_combinado_correct_dtype['aeropuerto'] == aeropuerto_base)\n",
        "]\n",
        "\n",
        "num_vuelos = df_vuelos_pre_final.shape[0]\n",
        "print(f\"El número de vuelos en el dataset filtrado es de: {num_vuelos}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hq9DCO1ja9eK"
      },
      "outputs": [],
      "source": [
        "df_vuelos_pre_final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q8sNCRjH_-8z"
      },
      "outputs": [],
      "source": [
        "# Evolución de pasajeros en el tiempo\n",
        "\n",
        "df_vuelos_pre_final.groupby('fecha_utc')['pasajeros'].sum().plot()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EdrU3HJhcsbt"
      },
      "outputs": [],
      "source": [
        "df_final = df_vuelos_pre_final[['fecha_utc', 'pasajeros']].copy()\n",
        "df_final.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mY8RnbBMo-sV"
      },
      "outputs": [],
      "source": [
        "df_final.dtypes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-IGkrSTfyoGz"
      },
      "source": [
        "#### 1.2.1) Analizamos la Serie Temporal"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7uPL_qsO4M4r"
      },
      "outputs": [],
      "source": [
        "# Transformamos la data a frecuencia semanal, agrupamos\n",
        "df_monthly = df_final.resample('M', on='fecha_utc').sum()\n",
        "\n",
        "# Generamos un rango mensual para el índice\n",
        "monthly_index = pd.date_range(start='2021-01-01', end='2024-04-01', freq='MS')\n",
        "\n",
        "# Nos aseguramos de que la longitud del nuevo índice matchee con la de nuestro Dataframe\n",
        "if len(monthly_index) == len(df_monthly):\n",
        "    df_monthly.index = monthly_index\n",
        "else:\n",
        "    raise ValueError(f\"Length mismatch: The new index has {len(monthly_index)} elements, but the DataFrame has {len(df_monthly)} rows.\")\n",
        "\n",
        "print(df_monthly)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ukq4oOm7Pt9"
      },
      "outputs": [],
      "source": [
        "# Descomponemos la serie temporal\n",
        "decomposition = seasonal_decompose(df_monthly['pasajeros'], model='multiplicative')\n",
        "\n",
        "# Graficamos\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "plt.subplot(411)\n",
        "plt.plot(df_monthly['pasajeros'], label='Original')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.subplot(412)\n",
        "plt.plot(decomposition.trend, label='Tendencia')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81t61FbO8gY1"
      },
      "source": [
        "**Interpretación**\n",
        "\n",
        "* **Original**\n",
        "\n",
        " La gráfica muestra una tendencia general al alza, con fluctuaciones evidentes que podrían indicar un patrón estacional.\n",
        "\n",
        "* **Tendencia**\n",
        "\n",
        " La línea de tendencia muestra un aumento constante en el número de pasajeros a lo largo del tiempo. Esto sugiere que la demanda general de vuelos está creciendo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "im1YM8tQ7pau"
      },
      "outputs": [],
      "source": [
        "# Plot the decomposed components\n",
        "plt.figure(figsize=(15, 12))\n",
        "\n",
        "plt.subplot(413)\n",
        "plt.plot(decomposition.seasonal, label='Estacionalidad')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.subplot(414)\n",
        "plt.plot(decomposition.resid, label='Residuos')\n",
        "plt.legend(loc='upper left')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu8KpX3E8zWj"
      },
      "source": [
        "**Interpretación**\n",
        "\n",
        "* **Estacionalidad**\n",
        "\n",
        " La gráfica estacional muestra picos y valles periódicos dentro de cada año. Esto sugiere un patrón estacional donde el número de pasajeros aumenta y disminuye regularmente en ciertas épocas del año. Por ejemplo, los picos podrían corresponder a temporadas de vacaciones u otros períodos de alta demanda, mientras que los valles podrían coincidir con períodos de baja demanda.\n",
        "\n",
        "* **Residuos**\n",
        "\n",
        " La gráfica de residuos muestra las fluctuaciones restantes que no siguen un patrón de tendencia o estacionalidad. Estas podrían deberse a factores aleatorios, eventos especiales o anomalías. Idealmente, los residuos no deberían mostrar ningún patrón, lo que indicaría que el modelo ha capturado bien las partes sistemáticas de los datos."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kR38Qn62yoG1"
      },
      "outputs": [],
      "source": [
        "# Establecemos 'fecha_utc' como índice\n",
        "df_analysis = df_final.copy()\n",
        "df_analysis.set_index('fecha_utc', inplace=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93bcXxSXyoG1"
      },
      "outputs": [],
      "source": [
        "plot_acf_pacf(df_analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JTPn5qiDyoG1"
      },
      "source": [
        "**Interpretación**\n",
        "\n",
        "En los gráficos ACF y PACF de la serie temporal diferenciada:\n",
        "\n",
        "- **ACF:** Observamos una disminución gradual de la autocorrelación a medida que aumentan los retardos, indicando que la serie tiene una dependencia temporal significativa, incluso después de la diferenciación.\n",
        "- **PACF:** Los primeros retardos muestran valores significativos, pero después de unos pocos retardos, la correlación cae cerca de cero, sugiriendo que la influencia directa de los retardos se limita a los primeros lags.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cmnBh9o2yoG1"
      },
      "outputs": [],
      "source": [
        "plot_fac(df_analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_C-XOrFyoG1"
      },
      "source": [
        "**Análisis de FAC de la Serie Temporal Diferenciada**\n",
        "\n",
        "**Propósito del Análisis:**\n",
        "\n",
        "Buscamos entender la autocorrelación presente en la serie temporal después de la diferenciación para determinar si la serie es estacionaria e identificar patrones de dependencia temporal.\n",
        "\n",
        "**Interpretación del Gráfico FAC**\n",
        "- **Primera Observación Alta:** Indica una fuerte autocorrelación en el primer retardo, común en series diferenciadas.\n",
        "- **Disminución Gradual:** La autocorrelación disminuye gradualmente, indicando una estructura de dependencia a lo largo del tiempo.\n",
        "- **Valores Extremos:** Los picos en los extremos pueden indicar estacionalidad o cambios abruptos en la serie.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KVJAYuLKyoG2"
      },
      "source": [
        "#### 1.2.2) Pruebas estadísticas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "shW5lptwyoG2"
      },
      "source": [
        "El objetivo principal es determinar si una serie temporal es estacionaria. Para lograrlo, vamos a utilizar tres pruebas estadísticas diferentes. La prueba de Zivot-Andrews se centrará en identificar una tendencia significativa en la media de la serie, mientras que la prueba de Dickey-Fuller Aumentada (ADF) nos ayudará a examinar la estacionariedad en la media. Por otro lado, la prueba de KPSS se enfocará en la estacionariedad en la varianza de la serie. Al realizar estas pruebas, buscamos comprender la naturaleza de la serie y determinar si son necesarias transformaciones o modelos específicos antes de proceder con su análisis y predicción."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HRHnlpjtyoG2"
      },
      "outputs": [],
      "source": [
        "stationarity_tests(df_analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LpJhm6pZyoG2"
      },
      "source": [
        "**Conclusiones**\n",
        "\n",
        "Los resultados de la prueba ADF sugieren que la serie es estacionaria, ya que su valor p es menor que 0.05 y su estadística de prueba es menor que los valores críticos. Por otro lado, la prueba KPSS también indica estacionariedad, ya que su valor p es menor que 0.05 y su estadística de prueba excede los valores críticos. En conclusión, ambas pruebas respaldan la estacionariedad de la serie temporal."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wOpU3dCLyoG2"
      },
      "outputs": [],
      "source": [
        "test_stationarity(df_analysis)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh8PdC6wyJUT"
      },
      "source": [
        "**Conclusiones:**\n",
        "\n",
        "Comprobamos mediante el test de Dickey-Fuller que nuestra serie es estacionaria. El valor de Test Statistic es menor que todos los valores críticos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpdiBxvWu0td"
      },
      "source": [
        "## 2. Análisis de importancia de Features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kfLZVUQvD3d"
      },
      "source": [
        "###2.1 Estudio PCA"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SObn33z5uzon"
      },
      "outputs": [],
      "source": [
        "df_argentina_españa_comerciales.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoJ904MqJstT"
      },
      "outputs": [],
      "source": [
        "df_argentina_españa_comerciales['pax'] = df_argentina_españa_comerciales.pax.astype('int64')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCAUyN3AJxwI"
      },
      "outputs": [],
      "source": [
        "# Hago dummies para aquellas features las cuales pueden categorizarse\n",
        "dummies_columns = ['tipo_de_movimiento', 'aerolinea_nombre', 'aeronave']\n",
        "\n",
        "# TODO - cambiar por oneHotEncoder\n",
        "df_argentina_españa_comerciales_dummies = pd.get_dummies(df_argentina_españa_comerciales, columns= dummies_columns)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fS14lxrqJ0Hp"
      },
      "outputs": [],
      "source": [
        "df_argentina_españa_comerciales_dummies.dtypes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MlcpdEXjz2DC"
      },
      "outputs": [],
      "source": [
        "# Separar las variables numéricas\n",
        "type_numeric_list = ['bool', 'datetime64']\n",
        "drop_train_cols_list = ['pasajeros', 'pax']\n",
        "\n",
        "df_train = df_argentina_españa_comerciales_dummies.select_dtypes(include= type_numeric_list)\n",
        "\n",
        "# separo los datos de train, que abarcan los años 2021 hasta 2023, y test a usar el 2024\n",
        "df_train = date_filter(df_train, 'fecha_utc', lim_inferior_entrenamiento, lim_superior_entrenamiento)\n",
        "df_test = date_filter(df_train, 'fecha_utc', lim_inferior_prueba, lim_superior_prueba)\n",
        "\n",
        "df_train_pca = df_train.copy()\n",
        "df_train_pca['fecha_int64'] = df_train_pca.fecha_utc.apply(lambda x: x.strftime('%Y%m%d')).astype('int64')\n",
        "df_train_pca.drop(columns= ['fecha_utc'], inplace= True)\n",
        "\n",
        "# Escalar las variables numéricas\n",
        "scaler = StandardScaler()\n",
        "X_numerico_scaled = scaler.fit_transform(df_train_pca)\n",
        "\n",
        "# Aplicar PCA\n",
        "pca = PCA()\n",
        "X_pca = pca.fit(X_numerico_scaled)\n",
        "\n",
        "# Calcular la varianza explicada por cada componente principal\n",
        "explained_variance_ratio = pca.explained_variance_ratio_\n",
        "\n",
        "# Imprimir la varianza explicada acumulada\n",
        "print(\"Varianza explicada acumulada por componente principal:\")\n",
        "print(explained_variance_ratio.cumsum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oHO97LLdS6ul"
      },
      "outputs": [],
      "source": [
        "# Imprimir la varianza explicada acumulada\n",
        "cumulative_variance = explained_variance_ratio.cumsum()\n",
        "print(\"Varianza explicada acumulada por componente principal:\")\n",
        "print(cumulative_variance)\n",
        "\n",
        "# Determinar el número de componentes necesarios para explicar el 99.99% de la varianza\n",
        "num_components = np.argmax(cumulative_variance >= 0.9999) + 1\n",
        "print(f\"Se requieren las primeras {num_components} componentes principales para explicar el 99.99% de la varianza.\")\n",
        "\n",
        "# Transformar los datos usando solo las componentes necesarias\n",
        "pca = PCA(n_components=num_components)\n",
        "X_pca_reduced = pca.fit_transform(X_numerico_scaled)\n",
        "\n",
        "# Obtener las cargas de los componentes\n",
        "component_loadings = pca.components_\n",
        "\n",
        "# Crear un DataFrame para las cargas\n",
        "loadings_df = pd.DataFrame(component_loadings.T, columns=[f'PC{i+1}' for i in range(num_components)], index=df_train_pca.columns)\n",
        "\n",
        "# Identificar las características más importantes para cada componente\n",
        "important_features = {}\n",
        "for i in range(num_components):\n",
        "    component = f'PC{i+1}'\n",
        "    sorted_loadings = loadings_df[component].abs().sort_values(ascending=False)\n",
        "    important_features[component] = sorted_loadings.index.tolist()\n",
        "\n",
        "# Encontrar características comunes en las 21 componentes principales\n",
        "common_features = set(important_features['PC1'])\n",
        "for i in range(2, num_components + 1):\n",
        "    common_features.intersection_update(important_features[f'PC{i}'])\n",
        "\n",
        "print(\"\\nCaracterísticas comunes en las 21 componentes principales:\")\n",
        "print(common_features)\n",
        "\n",
        "# Graficar la varianza explicada\n",
        "plt.figure(figsize=(6, 5))\n",
        "plt.plot(range(len(explained_variance_ratio)), explained_variance_ratio, '-o', label='Componente individual')\n",
        "plt.plot(range(len(explained_variance_ratio)), cumulative_variance, '-s', label='Acumulado')\n",
        "plt.axvline(x=num_components-1, color='r', linestyle='--', label=f'{num_components} componentes')\n",
        "plt.ylabel('Porcentaje de Varianza Explicada')\n",
        "plt.xlabel('Componentes Principales')\n",
        "plt.ylim(0, 1.05)\n",
        "plt.xticks(range(len(explained_variance_ratio)))\n",
        "plt.legend(loc='best')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NoLW1ivxU8NT"
      },
      "source": [
        "No es posible hacer una limpieza más exhaustiva que la realizada en el EDA con el método PCA."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "cTolPcAuQn5n"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vevdcaK42lWO"
      },
      "source": [
        "## 3. Definimos Dataset final para Entrenamiento"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JuZphGSG2kkC"
      },
      "outputs": [],
      "source": [
        "df_final_entrenamiento = df_vuelos_pre_final[['fecha_utc', 'pasajeros']].copy()\n",
        "\n",
        "# Transformamos la data a frecuencia semanal, agrupamos\n",
        "df_final_mensual_entrenamiento = df_final_entrenamiento.resample('M', on='fecha_utc').sum()\n",
        "\n",
        "# Generamos un rango mensual para el índice\n",
        "monthly_index = pd.date_range(start='2021-01-01', end='2024-04-01', freq='MS')\n",
        "\n",
        "# Nos aseguramos de que la longitud del nuevo índice matchee con la de nuestro Dataframe\n",
        "if len(monthly_index) == len(df_final_mensual_entrenamiento):\n",
        "    df_final_mensual_entrenamiento.index = monthly_index\n",
        "else:\n",
        "    raise ValueError(f\"Length mismatch: The new index has {len(monthly_index)} elements, but the DataFrame has {len(df_final_mensual_entrenamiento)} rows.\")\n",
        "\n",
        "df_final_mensual_entrenamiento.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tUXpUuz3Qwwe"
      },
      "source": [
        "## 4. Entrenamiento"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOfUd-74Qwwe"
      },
      "source": [
        "## 4.1) Entrenamos con ExponentialSmoothing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx5KRE_FlXWz"
      },
      "source": [
        "#### 4.1.1) Sin estacionalidad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XX-7P-zDQwwf"
      },
      "outputs": [],
      "source": [
        "# Obtenemos los primeros 3 años de datos\n",
        "train_start_date_se = df_final_mensual_entrenamiento.index[0]  # Fecha del primer dato\n",
        "train_end_date_se  = train_start_date_se + pd.DateOffset(years=3)  # Fecha después de 3 años\n",
        "\n",
        "train_se = df_final_mensual_entrenamiento.loc[train_start_date_se:train_end_date_se]\n",
        "\n",
        "# Entenamiento del modelo\n",
        "exp_model_se = ExponentialSmoothing(train_se, trend='mul')\n",
        "exp_model_fit_se = exp_model_se.fit()\n",
        "\n",
        "# Pronóstico\n",
        "forecast_steps_se = 12 # Por ejemplo, pronosticar un año después del entrenamiento\n",
        "forecast_start_date_se = train_end_date_se + pd.DateOffset(months=0)  # Fecha de inicio para pronosticar\n",
        "forecast_end_date_se = forecast_start_date_se + pd.DateOffset(months=forecast_steps_se - 1)  # Fecha de finalización para pronosticar\n",
        "forecast_se = exp_model_fit_se.predict(start=forecast_start_date_se, end=forecast_end_date_se)\n",
        "\n",
        "# Graficamos\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.plot(df_final_mensual_entrenamiento.index, df_final_mensual_entrenamiento.pasajeros, label='Datos', color='blue')\n",
        "plt.plot(train_se.index, exp_model_fit_se.fittedvalues, label='Ajuste (Entrenamiento)', color='green')\n",
        "plt.plot(forecast_se.index, forecast_se, label='Pronóstico', color='red')\n",
        "plt.title('Modelo de Suavizado Exponencial')\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Número de pasajeros')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOOmYptXQwwf"
      },
      "outputs": [],
      "source": [
        "train_forecast_se = exp_model_fit_se.predict(start=0, end=36)\n",
        "test_forecast_se = exp_model_fit_se.predict(start=36, end=40)\n",
        "\n",
        "test_se = df_final_mensual_entrenamiento[-5:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvWUAQY5lkdy"
      },
      "source": [
        "##### Analizamos métricas de performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "47FkAN_7Qwwg"
      },
      "outputs": [],
      "source": [
        "print('RMSE train: ', np.sqrt(mean_squared_error(train_se, train_forecast_se)))\n",
        "print('RMSE test: ', np.sqrt(mean_squared_error(test_se, test_forecast_se)))\n",
        "print('MAE train: ', mean_absolute_error(train_se, train_forecast_se))\n",
        "print('MAE test: ', mean_absolute_error(test_se, test_forecast_se))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y4mPoRzVmT0M"
      },
      "source": [
        "##### Analizamos residuos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lZw4Kak_mTAf"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(35,10))\n",
        "plt.plot(exp_model_fit_se.resid)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observaciones\n",
        "\n",
        "Se observa un comportamiento aleatorio en los residuos, lo que implica que no hay patrones presentes. Esto sugiere que el modelo entrenado ha capturado adecuadamente las tendencias y estacionalidades relevantes de los datos."
      ],
      "metadata": {
        "id": "EOBCoC7m02P3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmARnVtel2xb"
      },
      "source": [
        "### 4.1.2) Con estacionalidad"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DQYLWfzuQwwh"
      },
      "outputs": [],
      "source": [
        "# Obtenemos los primeros 3 años de datos\n",
        "train_start_date_ce = df_final_mensual_entrenamiento.index[0]  # Fecha del primer dato\n",
        "train_end_date_ce = train_start_date_ce + pd.DateOffset(years=3)  # Fecha después de 3 años\n",
        "\n",
        "train_ce = df_final_mensual_entrenamiento.loc[train_start_date_ce:train_end_date_ce]\n",
        "\n",
        "# Entenamiento del modelo\n",
        "exp_model_ce = ExponentialSmoothing(train_ce, seasonal='add', trend='mul', seasonal_periods=12)\n",
        "exp_model_fit_ce = exp_model_ce.fit()\n",
        "\n",
        "# Pronóstico\n",
        "forecast_steps_ce = 12 # Por ejemplo, pronosticar un año después del entrenamiento\n",
        "forecast_start_date_ce = train_end_date_ce + pd.DateOffset(months=0)  # Fecha de inicio para pronosticar\n",
        "forecast_end_date_ce = forecast_start_date_ce + pd.DateOffset(months=forecast_steps_ce - 1)  # Fecha de finalización para pronosticar\n",
        "forecast_ce = exp_model_fit_ce.predict(start=forecast_start_date_ce, end=forecast_end_date_ce)\n",
        "\n",
        "# Graficamos\n",
        "plt.figure(figsize=(35, 10))\n",
        "plt.plot(df_final_mensual_entrenamiento.index, df_final_mensual_entrenamiento.pasajeros, label='Datos', color='blue')\n",
        "plt.plot(train_ce.index, exp_model_fit_ce.fittedvalues, label='Ajuste (Entrenamiento)', color='green')\n",
        "plt.plot(forecast_ce.index, forecast_ce, label='Pronóstico', color='red')\n",
        "plt.title('Modelo de Suavizado Exponencial')\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Número de pasajeros')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ef9FCW91Qwwh"
      },
      "outputs": [],
      "source": [
        "train_forecast_ce = exp_model_fit_ce.predict(start=0, end=36)\n",
        "test_forecast_ce = exp_model_fit_ce.predict(start=36, end=40)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y-RCbfNaQwwi"
      },
      "outputs": [],
      "source": [
        "test_ce = df_final_mensual_entrenamiento[-5:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHDIObl8mLvt"
      },
      "source": [
        "##### Analizamos métricas de performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7V469MXRQwwi"
      },
      "outputs": [],
      "source": [
        "print('RMSE train: ', np.sqrt(mean_squared_error(train_ce, train_forecast_ce)))\n",
        "print('RMSE test: ', np.sqrt(mean_squared_error(test_ce, test_forecast_ce)))\n",
        "print('MAE train: ', mean_absolute_error(train_ce, train_forecast_ce))\n",
        "print('MAE test: ', mean_absolute_error(test_ce, test_forecast_ce))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r3ZQ379vmOFt"
      },
      "source": [
        "##### Analizamos residuos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "outputId": "761f0b03-2144-46bb-da9f-7d0745708a68",
        "id": "uF4F1J3XQwwj"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'exp_model_fit_ce' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-471f15dcd9ac>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m35\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_model_fit_ce\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'exp_model_fit_ce' is not defined"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 3500x1000 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "plt.figure(figsize=(35,10))\n",
        "plt.plot(exp_model_fit_ce.resid)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observaciones\n",
        "\n",
        "Se observa un comportamiento aleatorio en los residuos, lo que implica que no hay patrones presentes. Esto sugiere que el modelo entrenado ha capturado adecuadamente las tendencias y estacionalidades relevantes de los datos."
      ],
      "metadata": {
        "id": "UmJMmyOe0o5j"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VvrLmvBPQwwj"
      },
      "source": [
        "### 4.1.3) Hiperparámetros para ExponentialSmoothing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GSK5CCutQwwj"
      },
      "outputs": [],
      "source": [
        "alphas = betas = gammas = np.arange(0.10, 1, 0.20)\n",
        "abg = list(itertools.product(alphas, betas, gammas))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R4gdsOWTQwwj"
      },
      "outputs": [],
      "source": [
        "best_alpha, best_beta, best_gamma, best_mae = tes_optimizer(train_ce, test_ce, abg)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Yvlh8jbQwwj"
      },
      "outputs": [],
      "source": [
        "# Entrenar el modelo de suavizado exponencial triplano con hiperparámetros\n",
        "final_tes_model = ExponentialSmoothing(\n",
        "    train_ce,\n",
        "    trend=\"add\",\n",
        "    seasonal=\"add\",\n",
        "    seasonal_periods=12\n",
        ").fit(\n",
        "    smoothing_level=best_alpha,\n",
        "    smoothing_trend=best_beta,\n",
        "    smoothing_seasonal=best_gamma\n",
        ")\n",
        "\n",
        "# Obtener la fecha de inicio y fin del pronóstico\n",
        "forecast_start_date_hp = train_ce.index[-1] + pd.DateOffset(months=0)  # Comienza el mismo mes que en train_ce\n",
        "forecast_end_date_hp = forecast_start_date_hp + pd.DateOffset(months=11)  # Cubre un período de 12 meses\n",
        "\n",
        "# Generar el pronóstico\n",
        "y_pred_hp = final_tes_model.predict(start=forecast_start_date_hp, end=forecast_end_date_hp)\n",
        "\n",
        "# Visualizar los datos, ajuste del modelo sin hiperparámetros, pronóstico del modelo sin hiperparámetros y pronóstico del modelo con hiperparámetros\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.plot(df_final_mensual_entrenamiento.index, df_final_mensual_entrenamiento['pasajeros'], label='Datos', color='blue')\n",
        "plt.plot(train_ce.index, exp_model_fit_ce.fittedvalues, label='Ajuste (Modelo sin hiperparámetros)', color='green', linestyle=\":\")\n",
        "plt.plot(train_ce.index, final_tes_model.fittedvalues, label='Ajuste (Modelo con hiperparámetros)', color='orange')\n",
        "plt.plot(forecast_ce.index, forecast_ce, label='Pronóstico (Modelo sin hiperparámetros)', color='red', linestyle=\":\")\n",
        "plt.plot(y_pred_hp.index, y_pred_hp, label='Pronóstico (Modelo con hiperparámetros)', color='purple')\n",
        "plt.title('Comparación de Modelos de Suavizado Exponencial')\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Número de pasajeros')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "epBG6hbjmdiI"
      },
      "source": [
        "#### Analizamos métricas de performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WnQJ3F9rQwwk"
      },
      "outputs": [],
      "source": [
        "train_forecast_hp = final_tes_model.predict(start=0, end=36)\n",
        "test_forecast_hp = final_tes_model.predict(start=36, end=40)\n",
        "\n",
        "test_hp = df_final_mensual_entrenamiento[-5:]\n",
        "\n",
        "print('RMSE train: ', np.sqrt(mean_squared_error(train_ce, train_forecast_hp)))\n",
        "print('RMSE test: ', np.sqrt(mean_squared_error(test_hp, test_forecast_hp)))\n",
        "print('MAE train: ', mean_absolute_error(train_ce, train_forecast_hp))\n",
        "print('MAE test: ', mean_absolute_error(test_hp, test_forecast_hp))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "usvOVzyOmxsO"
      },
      "source": [
        "#### Analizamos residuos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a6ZPIcCWmxPa"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(35,10))\n",
        "plt.plot(final_tes_model.resid)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observaciones\n",
        "\n",
        "Se observa un comportamiento aleatorio en los residuos, lo que implica que no hay patrones presentes. Esto sugiere que el modelo entrenado ha capturado adecuadamente las tendencias y estacionalidades relevantes de los datos.\n",
        "\n"
      ],
      "metadata": {
        "id": "aIYW7CJ-0XKp"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2XuLtkX9Qwwl"
      },
      "source": [
        "## 4.2) Entrenamos con SARIMAX"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xIlBRUwsQwwl"
      },
      "source": [
        "En este modelo podemos sumar las dummies al análisis."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.1) Buscamos Hiperparámetros para SARIMAX"
      ],
      "metadata": {
        "id": "vQgnNek49M6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ts = pd.Series(df_final_mensual_entrenamiento['pasajeros'])\n",
        "p = d = q = range(0, 3)\n",
        "P = D = Q = range(0, 2)\n",
        "s = [12]  # Para datos mensuales\n",
        "\n",
        "mejor_modelo = busqueda_sarimax(ts, p, d, q, P, D, Q, s)\n",
        "print(\"Mejores parámetros del modelo SARIMAX:\", mejor_modelo)"
      ],
      "metadata": {
        "id": "npR0U1bv9dSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cWWgC--7m9Di"
      },
      "source": [
        "### 4.2.2) Con hiperparámetros, estacionalidad y sin dummies"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Guardar parámetros obtenidos en una variable\n",
        "mejores_params = mejor_modelo['params']\n",
        "\n",
        "# Dividir los datos: los primeros 36 meses para entrenamiento, el resto para prueba\n",
        "train_sm = df_final_mensual_entrenamiento.iloc[:36]\n",
        "test_sm = df_final_mensual_entrenamiento.iloc[36:48]\n",
        "\n",
        "# Ajustar el modelo SARIMAX en el conjunto de entrenamiento\n",
        "model_sm = SARIMAX(train_sm['pasajeros'], order=mejores_params[0], seasonal_order=mejores_params[1])\n",
        "results_sm = model_sm.fit()\n",
        "\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "predictions_sm = results_sm.get_forecast(steps=len(test_sm))\n",
        "predicted_mean_sm = predictions_sm.predicted_mean\n",
        "conf_int_sm = predictions_sm.conf_int()\n",
        "\n",
        "# Hacer predicciones en el conjunto de entrenamiento\n",
        "train_predictions_sm = results_sm.predict(start=train_sm.index[0], end=train_sm.index[-1])\n",
        "\n",
        "train_predictions_sm_to_plot = results_sm.predict(start=train_sm.index[0], end=test_sm.index[0]) # se crea para que grafique completo el entrenamiento\n",
        "\n",
        "# Graficar los datos originales, las predicciones y el intervalo de confianza\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.plot(df_final_mensual_entrenamiento.index, df_final_mensual_entrenamiento['pasajeros'], label='Datos reales', color='blue')\n",
        "plt.plot(test_sm.index, test_sm['pasajeros'], label='Datos de prueba', color='red')\n",
        "plt.plot(predicted_mean_sm.index, predicted_mean_sm, label='Predicciones (Prueba)', linestyle='--', color='purple')\n",
        "plt.plot(train_predictions_sm_to_plot.index, train_predictions_sm_to_plot, label='Predicciones (Entrenamiento)', linestyle='--', color='orange')\n",
        "plt.fill_between(predicted_mean_sm.index, conf_int_sm.iloc[:, 0], conf_int_sm.iloc[:, 1], color='k', alpha=0.1)\n",
        "plt.legend()\n",
        "plt.title('Modelo SARIMAX')\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Número de pasajeros')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OnHy9R3G9jct"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLewSZltnCzP"
      },
      "source": [
        "#### Analizamos métricas de performance"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Imprimir el resumen del modelo ajustado\n",
        "print(results_sm.summary())"
      ],
      "metadata": {
        "id": "otehk3ZD_VO5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfT_ezh3nHOL"
      },
      "outputs": [],
      "source": [
        "mae_test = mean_absolute_error(test_sm['pasajeros'], predicted_mean_sm)\n",
        "rmse_test = np.sqrt(mean_squared_error(test_sm['pasajeros'], predicted_mean_sm))\n",
        "mae_train = mean_absolute_error(train_sm['pasajeros'], train_predictions_sm)\n",
        "rmse_train = np.sqrt(mean_squared_error(train_sm['pasajeros'], train_predictions_sm))\n",
        "\n",
        "print(f'RMSE entrenamiento: {rmse_train}')\n",
        "print(f'RMSE prueba: {rmse_test}')\n",
        "print(f'Error Medio Absoluto prueba (MAE): {mae_test}')\n",
        "print(f'Error Medio Absoluto entrenamiento (MAE): {mae_train}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ck0L2-FNnHkA"
      },
      "source": [
        "#### Analizamos residuos"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vDUYtfqnONz"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(35,10))\n",
        "plt.plot(results_sm.resid)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observaciones\n",
        "\n",
        "Se observa un comportamiento random en los residuos, lo que implica que no hay patrones presentes. Esto sugiere que el modelo entrenado ha capturado las tendencias y estacionalidades relevantes de los datos."
      ],
      "metadata": {
        "id": "u7q9SYf_x4kE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2.3) Buscamos hiperparámetros con gridsearchCV en un pipeline"
      ],
      "metadata": {
        "id": "k489DKbaGhz4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "\n",
        "# Dividir los datos: los primeros 30 meses para entrenamiento, el resto para prueba\n",
        "train = df_final_mensual_entrenamiento.iloc[:30]\n",
        "test = df_final_mensual_entrenamiento.iloc[30:48]\n",
        "\n",
        "# Función para ajustar SARIMAX\n",
        "def sarimax_fit(X, y=None, order=(1, 0, 0), seasonal_order=(1, 1, 1, 12)):\n",
        "    model = SARIMAX(X, order=order, seasonal_order=seasonal_order)\n",
        "    results = model.fit(disp=False)\n",
        "    return results\n",
        "\n",
        "# Función para predecir usando un modelo ajustado de SARIMAX\n",
        "def sarimax_predict(results, steps):\n",
        "    forecast = results.get_forecast(steps=steps)\n",
        "    return forecast.predicted_mean\n",
        "\n",
        "# Función para hacer la predicción en el pipeline\n",
        "def predict_with_sarimax(X, order=(1, 0, 0), seasonal_order=(1, 1, 1, 12)):\n",
        "    model = sarimax_fit(X, order=order, seasonal_order=seasonal_order)\n",
        "    return sarimax_predict(model, steps=len(X))\n",
        "\n",
        "# Crear transformadores con FunctionTransformer\n",
        "fit_transformer = FunctionTransformer(sarimax_fit, kw_args={'order': (1, 0, 0), 'seasonal_order': (1, 1, 1, 12)}, validate=False)\n",
        "predict_transformer = FunctionTransformer(predict_with_sarimax, kw_args={'order': (1, 0, 0), 'seasonal_order': (1, 1, 1, 12)}, validate=False)\n",
        "\n",
        "# Definir el pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('fit', fit_transformer),\n",
        "    ('predict', predict_transformer)\n",
        "])\n",
        "\n",
        "# Definir el grid de parámetros\n",
        "param_grid = {\n",
        "    'fit__kw_args': [{'order': (1, 0, 0), 'seasonal_order': (1, 1, 1, 12)},\n",
        "                     {'order': (1, 1, 0), 'seasonal_order': (1, 0, 1, 12)},\n",
        "                     {'order': (0, 1, 1), 'seasonal_order': (0, 1, 1, 12)}]\n",
        "}\n",
        "\n",
        "# Definir el cross-validator\n",
        "tscv = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "# Definir el GridSearchCV\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=tscv, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Ajustar el GridSearchCV\n",
        "grid_search.fit(train['pasajeros'].values.reshape(-1, 1))\n",
        "\n",
        "# Mejor modelo encontrado\n",
        "best_params = grid_search.best_params_\n",
        "order = best_params['fit__kw_args']['order']\n",
        "seasonal_order = best_params['fit__kw_args']['seasonal_order']\n",
        "\n",
        "# Ajustar el mejor modelo en los datos de entrenamiento completos\n",
        "best_model_results = sarimax_fit(train['pasajeros'], order=order, seasonal_order=seasonal_order)\n",
        "\n",
        "# Hacer predicciones en el conjunto de prueba\n",
        "predicted_mean = sarimax_predict(best_model_results, steps=len(test))\n",
        "\n",
        "# Visualizar los resultados\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train.index, train['pasajeros'], label='Entrenamiento')\n",
        "plt.plot(test.index, test['pasajeros'], label='Prueba', color='orange')\n",
        "plt.plot(test.index, predicted_mean, label='Predicciones', color='green')\n",
        "plt.legend()\n",
        "plt.title('Predicción de pasajeros usando SARIMAX con GridSearchCV')\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Número de pasajeros')\n",
        "plt.show()\n",
        "\n",
        "mae = mean_absolute_error(test['pasajeros'], predicted_mean)\n",
        "print('mae', mae)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(test['pasajeros'], predicted_mean))\n",
        "print('rmse', rmse)\n",
        "\n",
        "# Mostrar los mejores parámetros encontrados\n",
        "print(\"Mejores parámetros:\", best_params)"
      ],
      "metadata": {
        "id": "JLulZ8eYGr_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "import numpy as np\n",
        "\n",
        "class SarimaxFit(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, order=(1, 0, 0), seasonal_order=(1, 1, 1, 12)):\n",
        "        self.order = order\n",
        "        self.seasonal_order = seasonal_order\n",
        "        self.results_ = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        self.model_ = SARIMAX(X, order=self.order, seasonal_order=self.seasonal_order)\n",
        "        self.results_ = self.model_.fit(disp=False)\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def get_results(self):\n",
        "        return self.results_\n",
        "\n",
        "class SarimaxPredict(BaseEstimator, TransformerMixin):\n",
        "    def __init__(self, steps=None):\n",
        "        self.steps = steps\n",
        "        self.forecast_ = None\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X, y=None):\n",
        "        if self.steps is None:\n",
        "            self.steps = len(X)\n",
        "        self.forecast_ = self.results_.get_forecast(steps=self.steps)\n",
        "        return self.forecast_.predicted_mean\n",
        "\n",
        "\n",
        "# Pipeline que ajusta y predice usando SARIMAX\n",
        "sarimax_pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),\n",
        "    ('sarimax_fit', SarimaxFit()),\n",
        "    ('sarimax_predict', SarimaxPredict())\n",
        "])\n",
        "\n",
        "param_grid = {\n",
        "    'sarimax_fit__order': [(1, 0, 0), (1, 1, 0), (2, 1, 0)],\n",
        "    'sarimax_fit__seasonal_order': [(1, 1, 1, 12), (1, 0, 1, 12)],\n",
        "    'sarimax_predict__steps': [10, 20, 30]\n",
        "}\n",
        "\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "\n",
        "# Configuración de la validación cruzada para series temporales\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "# Configuración de GridSearchCV\n",
        "grid_search = GridSearchCV(sarimax_pipeline, param_grid, cv=tscv, scoring='neg_mean_squared_error')\n",
        "\n",
        "# Datos de ejemplo (reemplaza con tus datos reales)\n",
        "X = np.random.rand(100)  # Serie temporal con 100 puntos\n",
        "\n",
        "X = y.reshape(-1, 1)\n",
        "\n",
        "# Ajustar el grid search\n",
        "grid_search.fit(X)\n",
        "\n",
        "# Mejor configuración de parámetros y mejor puntaje\n",
        "print(\"Mejores parámetros encontrados: \", grid_search.best_params_)\n",
        "print(\"Mejor puntaje (neg MSE): \", grid_search.best_score_)\n",
        "print(\"Mejor puntaje (MSE): \", -grid_search.best_score_)\n"
      ],
      "metadata": {
        "id": "p2e3EZr02dvP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
        "\n",
        "train = df_monthly.iloc[:30]\n",
        "test = df_monthly.iloc[30:48]\n",
        "\n",
        "# Función de ajuste\n",
        "def sarimax_fit(X, y=None, order=(1, 0, 0), seasonal_order=(1, 1, 1, 12)):\n",
        "    model = SARIMAX(X, order=order, seasonal_order=seasonal_order)\n",
        "    results = model.fit(disp=False)\n",
        "    return results\n",
        "\n",
        "# Función de prediccion\n",
        "def sarimax_predict(results, steps):\n",
        "    forecast = results.get_forecast(steps=steps)\n",
        "    return forecast.predicted_mean\n",
        "\n",
        "# Función de prediccion en el pipeline\n",
        "def predict_with_sarimax(X, order=(1, 0, 0), seasonal_order=(1, 1, 1, 12)):\n",
        "    model = sarimax_fit(X, order=order, seasonal_order=seasonal_order)\n",
        "    return sarimax_predict(model, steps=len(X))\n",
        "\n",
        "fit_transformer = FunctionTransformer(sarimax_fit, kw_args={'order': (1, 0, 0), 'seasonal_order': (1, 1, 1, 12)}, validate=False)\n",
        "predict_transformer = FunctionTransformer(predict_with_sarimax, kw_args={'order': (1, 0, 0), 'seasonal_order': (1, 1, 1, 12)}, validate=False)\n",
        "\n",
        "# Definir el pipeline\n",
        "pipeline = Pipeline([\n",
        "    ('fit', fit_transformer),\n",
        "    ('predict', predict_transformer)\n",
        "])\n",
        "\n",
        "\n",
        "order_1 = (1, 0, 0)\n",
        "order_2 = (1, 1, 0)\n",
        "order_3 = (0, 1, 1)\n",
        "\n",
        "# Definir el grid de parámetros\n",
        "param_grid = {\n",
        "    'fit__kw_args': [{'order': order_1, 'seasonal_order': (1, 1, 1, 12)},\n",
        "                     {'order': order_2, 'seasonal_order': (1, 0, 1, 12)},\n",
        "                     {'order': order_3, 'seasonal_order': (0, 1, 1, 12)}]\n",
        "}\n",
        "\n",
        "tscv = TimeSeriesSplit(n_splits=3)\n",
        "\n",
        "# Se define un GridSearchCV\n",
        "\n",
        "scoring_gs_cv = 'neg_mean_squared_error'\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=tscv, scoring= scoring_gs_cv)\n",
        "\n",
        "# Ajustar el GridSearchCV\n",
        "grid_search.fit(train['pasajeros'].values.reshape(-1, 1))\n",
        "\n",
        "# Mejor modelo encontrado\n",
        "best_params = grid_search.best_params_\n",
        "order = best_params['fit__kw_args']['order']\n",
        "seasonal_order = best_params['fit__kw_args']['seasonal_order']\n",
        "\n",
        "# Ajustar el mejor modelo en los datos de entrenamiento completos\n",
        "best_model_results = sarimax_fit(train['pasajeros'], order=order, seasonal_order=seasonal_order)\n",
        "\n",
        "# Se hace el predict\n",
        "predicted_mean = sarimax_predict(best_model_results, steps=len(test))\n",
        "\n",
        "# Indicadores de error\n",
        "mae = mean_absolute_error(test['pasajeros'], predicted_mean)\n",
        "print('mae', mae)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(test['pasajeros'], predicted_mean))\n",
        "print('rmse', rmse)\n",
        "\n",
        "# Mejores parámetros encontrados\n",
        "print(\"Mejores parametros:\", best_params)\n",
        "\n",
        "# Best model\n",
        "print(\"Mejores modelo:\", grid_search.best_estimator_)\n",
        "\n",
        "\n",
        "# Visualizar los resultados\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train.index, train['pasajeros'], label='Entrenamiento')\n",
        "plt.plot(test.index, test['pasajeros'], label='Prueba', color='orange')\n",
        "plt.plot(test.index, predicted_mean, label='Predicciones', color='green')\n",
        "plt.legend()\n",
        "plt.title('Predicción de pasajeros usando SARIMAX con GridSearchCV')\n",
        "plt.xlabel('Fecha')\n",
        "plt.ylabel('Número de pasajeros')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "Fg8zje_x894_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Análisis pronóstico de pasajeros y precio"
      ],
      "metadata": {
        "id": "v5Kw3-YLMNrt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Clasificación de precios de boletos:**\n",
        "\n",
        "Se determina la cantidad de pasajeros por año para cada mes. Según la cantidad máxima y mínima de ese año, definimos umbrales que nos permitan definir si la demanda es baja, media o alta. Esa demanda la extrapolamos a un precio bajo, medio o alto."
      ],
      "metadata": {
        "id": "ifh7KkRfMSq3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 6. Conclusiones"
      ],
      "metadata": {
        "id": "IU9lbDmsMhdx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nuestro objetivo es realizar pronósticos de vuelos desde Argentina a Madrid, con la finalidad de predecir cuándo sería más barato viajar en función de la cantidad de pasajeros que viajarían. En este contexto, las métricas de error son cruciales para asegurar la precisión y confiabilidad de nuestras predicciones.\n",
        "\n",
        "Después de evaluar las métricas de performance de los modelos entrenados, hemos decidido avanzar con la utilización del algoritmo Exponential Smoothing debido a su destacada performance. A continuación, se presentan las métricas obtenidas para este modelo:\n",
        "\n",
        "* RMSE (Root Mean Squared Error) en el conjunto de entrenamiento: 7545.16\n",
        "* RMSE en el conjunto de prueba: 2755.03\n",
        "* MAE (Mean Absolute Error) en el conjunto de entrenamiento: 5933.33\n",
        "* MAE en el conjunto de prueba: 2588.36\n",
        "\n",
        "Las razones principales para elegir el modelo de Exponential Smoothing son las siguientes:\n",
        "\n",
        "Precisión en el conjunto de prueba: El modelo de Exponential Smoothing ha demostrado tener los valores de RMSE y MAE más bajos en el conjunto de prueba comparado con los otros modelos entrenados. Esto indica que el modelo tiene una mejor capacidad para generalizar y predecir datos no vistos previamente.\n",
        "\n",
        "Consistencia en las métricas: Los valores de RMSE y MAE en el conjunto de entrenamiento también son relativamente bajos, lo que sugiere que el modelo está bien ajustado a los datos de entrenamiento sin sobreajustarse.\n",
        "\n",
        "Dado nuestro objetivo de predecir la demanda de vuelos para identificar períodos de menor costo de viaje, la precisión en las predicciones es fundamental. Las métricas de error, especialmente en el conjunto de prueba, son indicativas de la capacidad del modelo para hacer predicciones precisas. Un error más bajo en el conjunto de prueba sugiere que el modelo de Exponential Smoothing es más adecuado para realizar pronósticos fiables sobre la cantidad de pasajeros y, por ende, sobre los períodos más económicos para viajar."
      ],
      "metadata": {
        "id": "PlR2Wm1PMj9n"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bYiOfTmKFDfz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}